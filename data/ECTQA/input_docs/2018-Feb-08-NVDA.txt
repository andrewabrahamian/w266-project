I guess first question, when I think about normal seasonality for gaming, that would imply data center potentially north of $700 million-plus into the coming quarter. And so curious if I'm thinking about that right or whether crypto is being modeled more conservatively by you guys, and so would love to hear your thoughts there.
Which way is more conservatively, C.J.?
Yes? Sorry.
When you say conservatively, which direction were you saying it was. Are you implying up or down?
Well, just curious to hear your thoughts there.
We model crypto approximately flat.
Okay. And then I guess as part of a larger question, how are you thinking about seasonality for gaming into the ensuing quarter?
Well, there's a lot of dynamics going on in gaming. One dynamic, of course, is that there's a fairly sizable pent-up demand going into this quarter. But I think the larger dynamics that are happening relate to just the really amazing games that are out right now.
PUBG is just -- is doing incredibly well, as you might have known, and it's become a global phenomenon. And whether it's here in the United States or in Europe, or in China, in Asia, PUBG is just doing incredibly well. And we expect other developers to come up with similar genre, like PUBG, that are going to be coming out in the near future. And I'm super excited about these games.
And then, of course, there's Call of Duty, there's Star Wars. There's just so many great games that are out in the marketplace today, Overwatch and League of Legends, still doing well. There's just a countless number of great franchises that are out in the marketplace. And the gaming market is growing, and production value is going up. And that's driving increased unit sales of GPUs as well as ASPs of GPUs. And so I think those are -- that's probably the larger dynamic of gaming.
Your next question comes from the line of Mark Lipacis with Jefferies.
The first question, the checks we've done indicate that the Tensor Cores you put into Volta give it a huge advantage in neural network applications in the data center. And I'm wondering whether the Tensor Cores might also have a similar kind of utility in the gaming market.
Yes, first of all, I appreciate you asking a Tensor Core question. It is probably the single biggest innovation we had last year in data centers.
Our GPUs, the equivalent performance to one of our GPUs -- one of our Volta GPUs would take something along the lines of 20-plus CPUs or 10-plus nodes. And so 1 GPU alone would do deep learning so fast that it would take 10-plus CPU-powered server nodes to keep up with.
And then Tensor Core comes along last year, and we increased the throughput of deep learning, increased the computational throughput of deep learning by another factor of 8. And so Tensor Core really illustrates the power of GPUs. It's very unlike a CPU where the instruction set remains locked for a long time, and it's hard -- it's difficult to advance.
In the case of our GPUs and with CUDA, that's one of its fundamental advantages, we can continue to -- year in and year out, continue to add new capabilities to it. And so Tensor Core's boost of the original great performance of our GPU has really raised the bar last year.
And as Colette said earlier, our Volta GPU has now been adopted all over the world, whether it's in China with Alibaba, Tencent and Baidu, iFLYTEK, to here in the United States, Amazon and Facebook and Google and Microsoft and IBM and Oracle in Europe, in Japan. The number of cloud service providers that have adopted Volta has been terrific, and I think everybody really appreciates the work that we did with Tensor Core.
And all of the updates that are now coming out from the frameworks, Tensor Core is a new instruction set, it's a new architecture. And the deep learning developers have really jumped on it. And almost every deep learning framework is being optimized to take advantage of Tensor Core.
And on the inference side, and that's where it would play a role in video games, you could use deep learning now to synthesize and to generate new art. And we've been demonstrating some of that at GTC, if you've seen some of that. Whether it's improve the quality of textures, generating artificial characters, animating characters, whether it's facial animation with -- for speech or body animation, the type of work that you can do with deep learning for video games is growing. And that's where Tensor Core could be a real advantage.
If you take a look at the computational capability that we have in Tensor Core, compare that to a nonoptimized GPU or even a CPU, it's now 2-plus orders of magnitude greater computational throughput. And that allows us to do things like synthesize images in real time, synthesize virtual worlds, animate characters, animate faces, bring a new level of virtual reality and artificial intelligence to these video games.
Your next question comes from the line of Vivek Arya with Bank of America.
Congratulations on the strong growth and the consistent execution. Jensen, just a near- and longer-term question on the data center. Near term, you had, had a number of strong quarters in data center. How is the utilization of these GPUs? And how do you measure whether you're over or under from a supply perspective?
And then longer term, there seems to be a lot of money going into startups developing silicon for deep learning. Is there any advantage they have in taking a clean-sheet approach? Or is GPU the most optimal answer? Like if you were starting a new company looking at AI today, would you make another GPU? Or would you make another ASIC or some other format? Just any color would be helpful.
Sure. In the near term, the best way to measure customers that are already using our GPUs for deep learning is repeat customers. When they come back another quarter, another quarter, and they continue to buy GPUs, that would suggest that their workload has continued to increase.
The -- with existing customers that already have a very deep penetration, another opportunity for us would be using our GPUs for inference, and that's an untapped growth opportunity for our company that's really, really exciting, and we're seeing traction there.
For companies that are not at the forefront, the absolute forefront, of deep learning, which -- with the exception of 1 or 2 or 3 hyperscalers, almost everybody else I would put in this category, and their deployment, their adoption of deep learning applying deep learning to all of their applications is still ongoing. And so I think the second wave of customers is just showing up.
And then there's the third wave of customers which is -- they're not hyperscalers, they -- they're Internet service applications, Internet applications for consumers. They have enormous customer bases and -- that they could apply artificial intelligence to. But they run their application in hyperscale clouds. That third phase of growth is now really spiking, and I'm excited about that.
And so that's kind of the way to think about it. There's the pioneers, the first phase, are the returning customers. Then there's the second phase that's now ramping. The third phase that's now ramping. And then for everybody, we have an opportunity to apply our GPUs for inference.
If I had all the money in the world and I had, for example, billions and billions of dollars of R&D, I would give it to NVIDIA's GPU team, which is exactly what I do. And the reason for that is because the GPU was already inherently the world's best high-throughput computational processor.
A high-throughput processor is a lot more complicated than linear algebra done that you instantiate from a synopsis tool, it's not quite that easy. The computation throughput, keeping everything moving through your chip with supreme levels of energy efficiency with all of the software that's needed to keep the data flowing, with all of the optimizations that you do with each and every one of the frameworks, the amount of complexity there is just really enormous.
The networks are changing all the time. It started out with just basically CNNs, and then all kinds of versions of CNNs now. It started out with RNNs and simple RNNs, and now there's all kinds of LSTMs and gated RNNs, and all kinds of interesting networks that are growing. It started out with just 8 layers, and now it's 152 layers going to 1,000 layers. It started with mostly recognition, and now it's moving to synthesis with GANs. And there's so many versions of GANs.
And so all of these different types of networks are really, really hard to nail down. And we're still at the beginning of AI. So the ability for our GPUs to be programmable to all of these different architectures and networks is just an enormous advantage. You don't ever have to guess whether NVIDIA GPUs could be used for one particular network or another. And so you could buy our GPUs at will and know that every single GPU that you buy gives you an opportunity to reduce the number of servers in your data center by 22 nodes, by 10 nodes, 22 CPUs. And so the more GPUs you buy, the more money you save.
And so I think that capability is really quite unique. And then if I could just give you one example from last year or from previous year, we introduced 16-bit mix precision, we introduced 8-bit integer, we introduced NVLink the year before this last year. This year -- this last year, we introduced Tensor Core, which increased it by another factor of nearly 10. Meanwhile, our GPUs get more complex, energy-efficient. Efficiency gets better and better every single year, and the software richness gets more amazing.
And so it's a much harder problem than just a multiply accumulator. Artificial intelligence is the single most complex mode of software that the world has ever known. That's the reason why it's taken us so long to get here. And these high-performance supercomputers is an essential ingredient and an essential instrument in advancing AI. And so I don't think it's nearly as simple as linear algebra. But if I had all the money in the world, I would invest it in the team that we have.
Your next question come from the line of Stacy Rasgon with Bernstein Research.
I have a question for Colette. So if I correct for the Switch revenue growth in the quarter, it means the gaming business [x], which was up, I don't know maybe $140 million, $150 million. In your Q3 commentary, you did not call out crypto as a driver, you are calling it out in Q4. Is it fair to say that like that incremental growth is all crypto?
And I guess going forward, you mentioned pent-up demand. Normally, your seasonality for gaming will be down probably double digits. Do you think that pent-up demand is enough to reverse that normal seasonal pattern -- or normally down? And frankly, do you think gamers can even find GPUs at retail at this point to buy in order to satisfy that pent-up demand?
So let me comment on the first one. We did talk about our overall crypto business last quarter as well. We indicated how much we had in OEM boards, and we also indicated that there was definitely some also in our GTX business.
Keep in mind, that's very difficult for us to quantify down to the end customer. It is. But yes, there is also some in our Q3, and we did comment on it. So here we are commenting in terms of what we saw in terms of Q4. It's up a bit from what we saw in Q3, and we do again expect probably going forward. I'll let Jensen answer regarding the demand for gamers as we move forward.
Yes. So if you -- one way to think about the pent-up demand is we typically have somewhere between 6 to 8 weeks of inventory in the channel. And I think you would ascertain that globally right now the channel is relatively lean.
We're working really hard to get GPUs down to the marketplace for the gamers, and we're doing everything we can to advise Etailers and system builders to serve the gamers. And so we're doing everything we can. But I think the most important thing is we just got to catch up with supply.
Your next question comes from the line of Mitch Steves with RBC.
I actually want to circle back on the autos, since I was at CES. So it's still kind of on track for calendar -- towards calendar year '19, at the end of that, where we see the autonomous kind of ASP uplift. And just to clarify, the expected ASP uplift is somewhere around $1,000. Is that about right?
Yes, it just depends on mix. I think the -- for autonomous vehicles that still have drivers, passenger cars, branded cars, ASPs anywhere from $500 to $1,000 make sense. For robot taxis, where they're driverless, they're not autonomous vehicles, they're actually driverless vehicles, the ASP will be several thousand dollars.
And in terms of timing, I think that you're going to see larger and larger deployments starting this year and going through next year for sure, especially with robot taxis. And then with autonomous vehicles, cars that have autonomous driving capability, automatic driving capability starts late 2019. You could see a lot more in 2020. And just almost every premium car by 2022 will have autonomous automatic driving capabilities.
Your next question come from the line of Toshiya Hari with Goldman Sachs.
Great. Jensen, I was hoping to ask a little bit about inferencing. How big was inferencing within data center in Q4 or fiscal '18? And more importantly, how do you expect it to trend over the next 12 to 18 months?
Yes, thanks a lot, Toshi. First of all, just a comment about inference. The way that it works is you take the output of these frameworks. And the output of these frameworks is a really complex, large computational graph.
When you think about these neural networks, and they have millions of parameters, millions of anything is very complex. And these parameters are waves and activation layers and -- activation functions, and there are millions of them. And it's millions of them that composes -- consists of this computational graph. And this computational graph has all kinds of interesting and complicated layers.
And so you take this computational graph that comes out of each one of these frameworks, and they're all different. They're in different formats, they're in different styles, they have different architectures. They're all different. And you take these computational graphs, and you have to find a way to compile it, to optimize this graph, to rationalize all of the things that you could combine and fold, reduce the amount of conflict across all of the resources that are in your GPUs -- or in your processor.
And these conflicts could be on-chip memory and register files and data paths, and it could be the fabric, it could be the frame buffer interface, it could be the amount of memory. I mean you got -- this computer is really complicated across all these different processors and the interconnect between GPUs, the network that connects multiple nodes. And so you've got to figure out what all these different conflicts are, resources are, and compile and optimize to take advantage of it to keep it moving all the time.
And so TensorRT is basically a very sophisticated optimizing graph compilation -- graph compiler. And it targets each one of our processors. The way it targets Xavier is different to the way it targets Volta, the way it targets our inference, the way it targets for low energy, for different precisions. All of that targeting is different. And so first of all, TensorRT, the software of inference, that's really where the magic is.
Then the second thing that we do, we optimize our GPUs for extremely high throughput and to support different precisions because some networks could afford to have 8-bit integer or even less, some really could barely get by with a 16-bit floating point and some, you really would like to keep it at 32-bit floating point so that you don't have to second-guess about any precision that you lost along the way. And so we created an architecture that consists of this optimizing graph, computational graph compiler, to processors that are very high throughput, that are mix precision. Okay, so that's kind of the background.
We start -- we've been sampling our Tesla P4, which is our data center inference processor, and I -- we're seeing just really exciting response. And this quarter, we started shipping. We -- looking outwards, my sense is that the inference market is probably about as large in the data centers as training. And the wonderful thing is everything that you train on our processor will inference wonderfully on our processors as well.
And the data centers are really awakening to the observation that the more GPUs they buy for offloading inference and training, the more money they save. And the amount of money they save is not 20% or 50%, it's factors of 10. The money savings for all of these data centers that are becoming increasingly capital constrained is really quite dramatic.
And then the other inference opportunity for us is autonomous machines, which is self-driving cars. TensorRT also targets Xavier. TensorRT targets our Pegasus robot taxi computer. And they all have to inference incredibly efficiently so that we can sustain real time, keep the energy level low and keep the cost low for car companies, okay? So I think inference is a very important work for us. It is very complicated work, and we're making great progress.
Your next question come from the line of Blayne Curtis with Barclays.
Just kind of curious, as you look at the Gaming business -- I've kind of lost track of what seasonality is. You clearly have a big ramp ahead of you. I'm kind of curious, as you think about Pascal versus seasonality ahead of Volta, if you can just kind of extrapolate as you look out into April and maybe July.
I -- well, we haven't announced anything for April or July. And so the best way to think about that is Pascal is the best gaming platform on the planet. It is the most feature-rich, the best software, the most energy-efficient. And from $99 to $1,000, you could buy the world's best GPUs, the most advanced GPUs. And if you buy Pascal, you know you've got the best.
Seasonality is a good question and increasingly because gaming is a global market and because people play games every day. It's just part of their life. There's no -- I don't think there's much seasonality in TV or books or music. People just -- whenever new titles come out, that's when a new season starts.
And so in China, there's iCafes and there's Singles' Day, November 11, there's Back to School in the United States, there's Christmas, there's Chinese New Year. Boy, there are so many seasons that it's kind of hard to imagine what the exact seasonality is anymore. And so hopefully, over time, it becomes less of a matter. But the most important thing is that we expect Pascal to continue to be the world's best gaming platform for the foreseeable future.
Your next question comes from the line of Harlan Sur with JPMorgan.
Congratulations on the solid results and the execution. I know somebody asked a question about inferencing for the data center markets. But on inferencing for embedded and Edge applications, on the software and firmware side, you talked about TensorRT framework; on the hardware side, you've got the Jetson TX platform; for embedded and Edge inferencing applications, things like drones and factory automation and transportation. What else is the team doing in the embedded market to capture more of the TAM opportunity there going forward?
Yes, thanks a lot, Harlan. The NVIDIA TensorRT is really the only optimizing inference compiler in the world today, and it targets all of our platforms. And we do inference in the data center that I mentioned earlier. In the embedded world, the first embedded platform we're targeting is self-driving cars.
In order to drive the car, you basically inference or try to predict or perceive what's around you all the time. And that's a very complicated inference matter. It could be extremely easy, like detecting the car in front of you and applying the brakes, or it could be incredibly hard which is trying to figure out whether you should stop at an intersection or not.
If you look at most intersections, you can't just look at the lights to determine where do you stop. There are very few lines. And so using scene understanding and using deep learning, we have the ability to recognize where to stop and whether to stop.
And then for Jetson, we have a platform called Metropolis. And Metropolis is used for very large scale smart cities where cameras are deployed all over to keep cities safe. And we've been very successful with smart cities. Just about every major smart city provider, and what is called intelligent video analysis company, whether -- almost all over the world is using NVIDIA's platform to do inference at the Edge, AI at the Edge.
And then we've announced recently success with FANUC, the largest manufacturing and robotics company in the world; Komatsu, one of the largest construction equipments company in the world to apply AI at the Edge for autonomous machines. Drones, we have several industrial drones that are inspecting pipelines and inspecting power lines, flying over large spans of farms to figure out where to spray insecticides more accurately. There's all kinds of applications.
So you're absolutely right that inference at the Edge or AI at the Edge is a very large market opportunity for us, and that's exactly why TensorRT was created.
Your next question come from the line of Joe Moore with Morgan Stanley.
You had mentioned how lean the channel is in terms of gaming cards. There's been an observable increase in prices at retail. And I'm just curious, is that a broad-based phenomenon? And is there any economic ramifications to you? Or is that just sort of retailers bringing prices up in a shortage environment?
We don't set prices at the end of the market. And the best way for us to solve this problem is work on demand -- excuse me, work on supply. The demand is great. And it's very likely the demand will remain great as we look throughout -- through this quarter.
And so we just have to keep working on increasing supply. We have -- our suppliers are the world's best and the largest semiconductor manufacturers in the world, and they're responding incredibly, and I'm really grateful for everything they're doing. We just got to catch up to that demand which is just really great.
Your next question comes from the line of Chris Rolland with Susquehanna.
Great quarter. So just to clarify, Jensen, on pent-up demand. One of your GPU competitors basically said that the constraint was memory. I just want to make sure that, that was correct.
And then in the CFO commentary, you mentioned opportunities for professional vis, like AI and deep learning. Can you talk about that, and what kind of applications you would use, Quadro versus Volta or GeForce?
Sure. We are -- we're just constrained. Obviously, we're 10x larger of a GPU supplier than the competition. And so we have a lot more suppliers supporting us and a lot more distributors taking our products to market and a lot more partners distributing our products all over the world. And so we -- I don't know how to explain it aside from the demand is just really great. And so we've just got to keep our nose to it and catch up to the demand.
With respect to Quadro, Quadro is a workstation processor. The entire software stack is designed for all of the applications that the workstation industry uses. And it's used -- the quality of the rendering is, of course, world-class because of NVIDIA and -- but the entire software stack has been designed so that mission-critical applications or long-life industrial applications and companies that are enormous and gigantic manufacturing and industrial companies in the world could rely on an entire platform which consists of processors and system and software and middleware and all the integrations into all of the CAD tools in the world to know that the supplier is going to be here and can be trusted for the entire life of the use of that product which could be several years, but the data that is generated from it has to be accountable for a couple of decades.
You need to be able to pull up an entire design of a plane or a train or a car a couple decades after it was sent to production to make sure that it's still in compliance, and if there are any questions about it, that it can be pulled up. NVIDIA's entire platform was designed to be professional class, professional grade, long lived.
Now the thing that's really exciting about artificial intelligence is we now can use AI to improve images. Like, for example, you could fix a photograph using AI. You could fill in damaged parts of a photograph or parts of the image that hasn't been rendered yet, you want to use AI to fill in the dots, predict the future, rendering results, which we announced and which we demonstrated at GTC recently.
You could use that to generate designs. You sketch up a few strokes of what you want a car to look like. And based on the inventory, safety, physics, it could -- it has learned how to fill in the rest of it, okay, design the rest of the chassis on your behalf. It's called generative design. We're going to see generative design in product design, in building design and just about everything.
The last, if you will, 90% of the work is after the initial inspiration or the conceptual design is done. That part of it can be highly automated through AI. And so Quadro could be used as a platform that designs as well as generatively designs. And then lastly, a lot of people are using our workstations to also train their neural networks for these generative designs. And so you could train and develop your own networks and then apply it in the applications, okay?
So AI, think of AI really as, in the final analysis, the future way of developing software. It's a brand-new capability where computers can write its own software. And the software that's written is so complex and so capable that no humans could write it ourselves. And so you could teach, you could use the data to teach a software to figure out how to write the software by itself.
And then when you're done developing that software, you could use it to do all kinds of stuff, including design products. And so for workstations, that's how it's used.
Your next question come from the line of Craig Ellis with B. Riley.
Congratulations on the very good quarterly execution. A lot of near-term items here on gaming. So I'll switch it to longer term. Jensen, at CES, I think you said that there are now 200 million GeForce users globally. And if my math is correct, then that would be up about 2x over the last 3 to 4 years.
So the question is, is there anything that you can see that would preclude that kind of growth over a similar period? And given the recent demand dynamics, I think we've seen that NVIDIA's direct channels have been very good sources for GPUs at the prices that you intend. So as we look ahead, should we expect any change in channel management from the company?
Yes. Thanks a lot, Craig. In the last several years, several dynamics happened at the same time. And all of it were the favorable contributions to today. First of all, gaming became a global market, and China became one of the largest gaming markets in the world.
But second, because the market became so big, developers could invest extraordinary amounts into the production value of a video game. They could invest a few hundred million dollars and know that they're going to get the return on it. Back when the video game industry was quite small or when PC industry -- PC gaming was small, it was too risky for a developer to invest that much.
And so now an investor, a developer could invest hundreds of millions of dollars and create something that is just completely photorealistic and immersive and just beautiful. And so the production -- when a production value goes up, the GPU technology that's needed to run it well goes up. It's very different than music, it's very different than watching movies. Everything in video games is synthesized in real time. And so when the production value goes up, the ASP or the technology has to go up.
And then lastly, the size of the market, people have wondered how big the video game market is going to be. And I've always believed that the video game market is going to be literally everyone. In 10 years' time, 15 years' time, there's going to be another 1 billion people on Earth. And those people are going to be gamers. We're going to see more and more gamers.
And not to mention that, almost every single sport could be a virtual-reality sport. So video games is every sport. So eSport can be any sport and every sport and every type of sport. And so I think when you consider this and put that in your mind, I think the opportunity for video games is going to be quite large, and that's essentially what we are seeing.
Your next question comes from the line of William Stein with SunTrust.
I'm hoping we can touch on automotive a little bit more. In particular, I think, in the past, you've talked about expecting sort of a low revenue growth in this market until roughly the 2020 time frame when autonomous driving kicks in, in a more meaningful way. But of course, you have the AI copilot that seems to be potentially ramping sooner, and you have at least 1 marquee customer that is ramping now, I guess, but volumes aren't quite that large on the autonomous driving side. So any guidance as to when we might see these 2 factors start to accelerate revenue in that end market?
Yes. Thanks a lot, Will. I wish I had more precision for you, but here are some of the dynamics that I believe in. I believe that autonomous capabilities -- autonomous driving, is the single greatest dynamic next to EVs in the automotive industry. And transportation is a $10 trillion industry. Between cars and shuttles and buses, delivery vehicles, I mean, it's just an extraordinary, extraordinary market. And everything that's going to move in the future will be autonomous. That's for sure. And it will be autonomous fully, or it will be autonomous partly. The size of this marketplace is quite large. In the near term, I -- our path to that future, which I believe starts in 2020 -- 2019, 2020, but starts very strongly in 2022, I believe the path to that, in our case, has several elements. The first element is that in order for all these companies, whether they're Tier 1s or startups or OEMs or taxi companies or ride-hailing companies or tractor companies or shuttle companies or pizza delivery shuttles, in order to deliver -- in order to create their autonomous driving capability, the first thing you have to do is train a neural network. And we've created a platform we call the NVIDIA DGX that allows everybody to train their neural networks as quickly as possible. So that's first. The development of the AI requires GPUs, and we benefit first from that. The second is -- which we'll start this year and next year, is development platforms for the cars themselves for the vehicles themselves. And finally, Xavier's here. We have first silicon at Xavier's, the most complex SOC that was ever made. And we're super excited about the state of Xavier, and we're going to be sampling it in Q1. And so now we'll be able to help everybody create development systems. And there'll be thousands and tens of thousands of quite expensive development systems based on Xavier and based on Pegasus that the world is going to need. And so that's the second element. The third element, in the near term, will be development agreements. Each one of these projects are engineering-intensive, and there's a development agreement that goes along with it. And so these 3 elements, these 3 components, are in the near term. And then hopefully, starting from 2019, going forward and very strongly going from 2022 and beyond, the actual car revenues and economics will show up. Appreciate that question. And I think this is our last question, yes?
Well, we had a record quarter, wrapping up a record year. We have a strong -- we had strong momentum in our gaming, AI, data center and self-driving car businesses. It's great to see adoption of NVIDIA's GPU computing platform increasing in so many industries. We accomplished a great deal this last year, and we have big plans for this coming year. Next month, the brightest minds in AI and the scientific world will come together at our GPU Technology Conference in San Jose. GTC has grown tenfold in the last 5 years. This year we expect more than 8,000 attendees. GTC is the place to be if you're an AI researcher or doing any field of science where computing is your essential instrument. There will be over 500 hours of talks of recent breakthroughs and discoveries by leaders in the field, such as Google, Amazon, Facebook, Microsoft and many others. Developers from industries ranging from health care to transportation to manufacturing and entertainment will come together and share state-of-the-art and AI. This is going to be a great GTC. I hope to see all of you there.
This concludes today's conference call. You may now disconnect. Thank you for your participation.